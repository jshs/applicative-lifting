\section{Lifting with Combinators}\label{sec:combinators}

\subsection{Motivation}\label{subsec:combinator-motivation}

The normalization approach to solving lifted equations works only if the
opaque terms on both sides coincide.
This is not true for all equations of interest.
Let's revisit the set version of addition of natural numbers, $\oplus$ from
Example~\ref{exmp:set-intro}.
This operator is also commutative, so it should be possible to prove
\[ X \oplus Y = Y \oplus X. \]
After unfolding and normalization, we get
\begin{equation}\label{eq:comb-intro1}
	\pure{(\abs{xy}{x + y})} \ap X \ap Y = \pure{(\abs{yx}{y + x})} \ap Y \ap X.
\end{equation}
Clearly, this can't be solved with a standard congruence rule, because we would
have to to prove that $X$ is equal to $Y$.
Since we are concerned with transferring properties from a base domain,
we don't want to assume anything about those opaque subterms.

Hinze showed that such equations can be solved if certain \emph{combinators}
can be lifted.
Informally, combinators are functions which rearrange their arguments in a
specific manner.
We have already used two combinators, $\mathbf{I}$ and $\mathbf{B}$.
Lifting their defining equations (see Table~\ref{tab:combinators}) gives us
the identity and composition laws, respectively.
If the lifted combinator performs the same rearrangement with arbitrary
functorial values, one can translate that particular term structure between the
two layers.
In this case, we simply say that the combinator \emph{exists}.
To continue with~\eqref{eq:comb-intro1}, we could attempt to change the order of
$Y$ and $X$ on the right-hand side.
Note that these appear as arguments to a pure function.
The $\mathbf{C}$ combinator, also known as `flip' in functional programming,
does what we want: $\mathbf{C}fxy = fyx$.
The lifted equation is
\begin{equation}\label{eq:flip-lifted}
	\pure \mathbf{C} \ap f \ap x \ap y = f \ap y \ap x,
\end{equation}
and it is indeed true for set idiom!
From this we get
\begin{equation}\label{eq:comb-intro2}
	\pure{(+)} \ap X \ap Y = \pure{(\mathbf{C}(+))} \ap X \ap Y.
\end{equation}
The right-hand side is no longer the normal form of $Y \oplus X$, but still
a canonical form (which is why we distinguish these two).
But now the argument lists on both sides coincide.
We reduce to
\[ \abs{xy}{x + y} = \abs{xy}{y + x}, \]
which is extensionally equivalent to the base equation $x + y = y + x$.
The availability of equation~\eqref{eq:flip-lifted} is a quite powerful
condition, because it will allow us to permute opaque terms freely.
If permutations exist such that both sides of an equation in canonical form
align regarding their opaque terms, reduction by congruence is possible again.
This will again lead to the expected base equation.
However, the combinator $\mathbf{C}$ does not exist for all applicative functors.
For example, the order of values in a state monad may be significant.

\begin{table}\centering
\begin{tabular}{cll}
Symbol & Reduction \\
\hline
$\mathbf{B}$ & $\mathbf{B} x y z = x (y z)$ \\
$\mathbf{I}$ & $\mathbf{I} x = x$ \\
\hline
$\mathbf{C}$ & $\mathbf{C} x y z = x z y$ \\
$\mathbf{K}$ & $\mathbf{K} x y = x$ \\
$\mathbf{W}$ & $\mathbf{W} x y = x y y$ \\
$\mathbf{S}$ & $\mathbf{S} x y z = x z (y z)$ \\
$\mathbf{H}$ & $\mathbf{H} x y z = x y (z y)$ \\
\end{tabular}
\caption{Useful combinators.}
\label{tab:combinators}
\end{table}

Combinators appeared originally in the context of logic~\cite{curry68}.
They were studied because it is possible to write logical formulas without
variables using only applications of suitable combinators, as opposed to the
usual lambda calculus.
Table~\ref{tab:combinators} lists all combinators which are used throughout
this text, together with their defining equations.
There are certain sets of combinators which are sufficient to express all
lambda terms, $\{\mathbf{S,K}\}$ being one of them.
In other sets, only a limited part of terms is representable.
Hinze's Lifting Lemma shows that all terms and thus all equations can be
lifted while preserving the variable structure if $\mathbf{S}$ and $\mathbf{K}$
exist.
He also notes that other combinator set are useful, because there are idioms
where more than $\{\mathbf{B,I}\}$, but not all combinators exist.
Generally speaking, additional combinators enlarge the set of equations which
can be lifted.

The original proof of the Lifting~Lemma~\cite[11--14]{hinze10} uses induction
on the structure of idiomatic terms; it is not entirely obvious how it can
be generalized to other combinators sets, as it depends on the availability
of $\mathbf{K}$ to lift tuple projections.
In this section we present an implementation of this generalized lifting,
whose underlying concept works with arbitrary combinators.
However, it depends on an abstraction algorithm and the structure of
representable terms, which are difficult to derive automatically.
Therefore we will restrict ourselves to certain sets (``bases'') with
fixed algorithms, while understanding that the scope can be extended 
if needed.

\subsection{Generic Lifting}\label{subsec:generic-lifting}

We start with the relationship of combinators and lambda terms.
The equations in Table~\ref{tab:combinators} can be expressed as abstractions
$\mathbf{I} = \abs{x}{x}$ etc.
If we substitute occurrences of combinators in a term (signified by $=_\delta$),
new abstractions are introduced, which may be beta-reduced afterwards:
\[ \mathbf{WB} =_\delta (\abs{fx}{fxx})(\abs{gfx}{g(fx)}) =_\beta \abs{xy}{x(xy)}. \]
The question arises when and how this process can be reversed, meaning that
all abstractions are replaced by suitable combinators.
In Curry et.~al.~\cite[Section~6A]{curry68}, terms with variables, but no
abstractions are considered.
A syntactical operation is defined, denoted $[x]t$, where $t$ is such a term
and $x$ is a variable.
The desired property is that $x$ does not occur in $[x]t$, and
$([x]t)x =_{\delta\beta} t$.
Due to its notation, the operation is known as \emph{bracket abstraction}.
There is an obvious correspondence with lambda abstractions $\abs{x}{t}$.
Bracket abstraction however stands for a concrete applicative term, whereas
a lambda is an object of the syntax itself.
Replacing lambdas $\abs{x}{t}$ by brackets $[x]t$ performs the shift to a
combinator representation.
Curry et.\ al. give several possible definitions for bracket abstraction.
They note that these follow a scheme they refer to as an algorithm---a sequence
of rules, where each rule is a partial definition.
The rules may invoke abstraction recursively.
In particular, the following rules are used:

\begin{alignat}{2}
	\tag{$i$} [x]x &= \mathbf{I}, && \\
	\tag{$k$} [x]t &= \mathbf{K} t &&\qquad\text{if $x$ not free in $t$}, \\
	\tag{$\eta$} [x]tx &= t &&\qquad\text{if $x$ not free in $t$}, \\
	\tag{$b$} [x]st &= \mathbf{B}s([x]t) &&\qquad\text{if $x$ not free in $s$}, \\
	\tag{$c$} [x]st &= \mathbf{C}([x]s)t &&\qquad\text{if $x$ not free in $t$}, \\
	\tag{$s$} [x]st &= \mathbf{S}([x]s)([x]t). &&
\end{alignat}

The algorithm which consists of rules $(i)$, $(k)$ and $(s)$, in that order,
is written succinctly as $(iks)$.
The algorithm attempts to use the rules in their left-to-right order, applying
the first one whose restrictions are satisfied by the term at hand.
Each abstraction algorithm $A$ introduces a certain set of basic combinators,
which we refer to as $C(A)$.
It is sound only if certain postulates about those combinators, which are again
the equations in Table~\ref{tab:combinators}, are assumed.

\begin{example}\label{exmp:bracket-abs}
Using the $(iks)$ algorithm, one gets
\[ [x]xxy \stackrel{(s)}{=} \mathbf{S}([x]xx)([x]y)
	\stackrel{(s),(k)}{=} \mathbf{S}(\mathbf{S}([x]x)([x]x))(\mathbf{K}y)
	\stackrel{(i)}{=} \mathbf{S}(\mathbf{SII})(\mathbf{K}y). \]
Attempting to use the $(ik\eta bc)$ algorithm with the same abstraction
quickly comes to a stop:
\[ [x]xxy \stackrel{(c)}{=} \mathbf{C}([x]xx)y, \]
which is undefined.
\end{example}

As we can see, not all algorithms are total.
Therefore, there is a trade-off between the combinators required and the terms
for which abstraction is possible.
Bunder~\cite{bunder96} presents an analysis of the situation for certain
algorithms and combinator sets, based on rigorous definitions for term
translation and definability.
We will come back to this later, when we discuss how to order the variables in
an idiomatic term such that abstraction is defined.
For now, the concept of bracket abstraction with the example of rules
$(i)$--$(s)$ is sufficient.

Next, we attempt to transfer these concepts to idiomatic terms.
On the one hand, this is quite intuitive since the latter are also formed by
an application operator, and pure terms can be identified with constants.
But we do not have any ``idiomatic abstractions''.
Hinze actually defines these in terms of abstract combinators and an
extensionality property of the idiom.
For our purpose it is sufficient to work directly with bracket abstraction,
and we assume that all combinators are lifted, i.e. expressible as a pure term.
To clarify the following discussion, we adjust our $\mathcal{I}$ formalism
and replace opaque terms $\sterm x$ with variables.

\begin{definition}
The set of generic idiomatic terms $\mathcal{I}'$ is defined by
\begin{equation}
	\mathcal{I}' ::= \sivar \mathcal{V} \mid \spure \mathcal{T} \mid
		\mathcal{I}' \sap \mathcal{I}'.
\end{equation}
We reuse the congruence $\simeq$ from Definition~\ref{def:idiomatic-terms} for
generic terms.
The set of variables $\vars(t)$ of $t$ is defined as the set of all arguments
to $\sivar$ occuring in $t$.
The sequence of variables $\varseq(t)$ is defined similarly to $\operatorname{opaq}$.
Unlifting (see Definition~\ref{def:unlifting}) is also transferred, but uses
the variable $x$ in subterms $\sivar x$ instead of inventing new ones.
\end{definition}

Using this definition, it is clear what the rules for idiomatic abstraction
are:
\begin{alignat}{2}
	\tag{$i'$} [x]'(\sivar x) &= \spure \mathbf{I}, && \\
	\tag{$k'$} [x]'t &= \spure \mathbf{K} \sap t &&\qquad\text{if } x \not\in \vars(t), \\
	\tag{$\eta'$} [x]'(t \sap \sivar x) &= t &&\qquad\text{if } x \not\in\vars(t), \\
	\tag{$b'$} [x]'(s \sap t) &= \spure \mathbf{B} \sap s \sap [x]'t &&\qquad\text{if } x \not\in \vars(s), \\
	\tag{$c'$} [x]'(s \sap t) &= \spure \mathbf{C} \sap [x]'s \sap t &&\qquad\text{if } x \not\in \vars(t), \\
	\tag{$s'$} [x]'(s \sap t) &= \spure \mathbf{S} \sap [x]'s \sap [x]'t. &&
\end{alignat}
In general, the algorithm $A'$ on idiomatic terms is obtained from algorithm
$A$ on regular terms by lifting its rules in this fashion, preserving order.

Before we show the connection to the canonical form, there is one thing which
remains to be considered.
The interchange law allows us to move a variable out of the left subterm of
an application, given that the right subterm is pure.
This is not captured by rules $(b')$ and $(i')$, which are the only ones from
above which are valid in all idioms.
We define a combinator $\mathbf{T}xy = yx$ and the rules
\begin{alignat}{2}
	\tag{$t$} [x]st &= \mathbf{T}t([x]s) &&\qquad\text{if $t$ contains no variables}, \\
	\tag{$t'$} [x]'(s \sap t) &= \spure \mathbf{T} \sap t \sap [x]'s
		&&\qquad\text{if } \vars(t) = \emptyset.
\end{alignat}
Soundness of rule $(t')$ can be shown to be equivalent to the interchange law.
It is important to understand that $\mathbf{T}$ does not have to exist in the
idiom; these rules do not fit exactly in the pattern of the other rules.
The $\mathbf{T}$ combinator is also necessary to formulate the most generic
rule for the $\mathbf{W}$ combinator.
Without the interchange law, it could only be used for terms
$t \sap \sivar x \sap \sivar x$, i.e. those where the same variable is applied
twice in direct succession.
In an idiom, there may be arbitrary pure terms ``inbetween'' the variables.
We use the $(w)$ rule when the variable occurs in both operands of an application,
just like the $(s)$ rule.
\begin{alignat}{2}
	\tag{$w$} [x]st &= \mathbf{W}(\mathbf{B}(\mathbf{T}[x]t)(\mathbf{B}\mathbf{B}[x]s))
		&&\qquad\text{if $[x]t$ contains no variables}.
\end{alignat}
$(w')$ is derived similarly to the other rules.

As with ordinary terms, we demand a soundness property for idiomatic bracket
abstraction, namely that $[x]'t' \sap \sivar x \simeq_C t'$ holds true.
The definitions for the additional combinators $C$ get lifted to
$\spure \mathbf{I} \sap x \simeq_C x$ and so on, consistently extending our
congruence relation $\simeq$ to $\simeq_C$.
% FIXME \simeq is not consistent -- pure I <> pure x ~= pure x and pure I <> pure x = pure (Ix)

\begin{lemma}\label{thm:bracket-lifting}
Let $t' \in \mathcal{I'}$ be a generic idiomatic term, and $x \in \mathcal{V}$
a variable.
For an abstraction algorithm $A'$ consisting of a subset of rules $(i')$--$(t')$,
we have $\unlift{[x]'t'} = [x]\unlift{t'}$ and $[x]'t' \sap \sivar x \simeq_{C(A')} t'$,
assuming that $[x]'t'$ is defined.
Also, bracket abstraction does not add variables:
$\vars([x]'t') \subseteq \vars(t')$.
\end{lemma}
\begin{proof}
This statement uses that fact that the rules in $A'$ are very similar to those
of $A$.
In particular, rule $(r')$ is applied first when evaluating $[x]'t'$ iff rule
$(r)$ is applied first to $[x]\unlift{t'}$.
The remainder of the proof is a simple induction.
We show the case involving rule $(c')$ as an example, the other cases are similar.
Thus $t = s \sap u$ with $x \not\in \vars(u)$.
The induction hypothesis is
\[ \unlift{[x]'s} = [x]\unlift{s} \quad\text{and}\quad [x]'s \sap \sivar x \simeq_{C(A')} s
	\quad\text{and}\quad \vars([x]'s) \subseteq \vars(s). \]
Then we have
\[\begin{split}
	\unlift{[x]'t} \stackrel{(c')}{=} \unlift{(\spure \mathbf{C} \sap [x]'s \sap u)}
	= \mathbf{C} (\unlift{[x]'s}) (\unlift{u}) \\
	\stackrel{\text{(IH)}}{=} \mathbf{C} ([x]\unlift{s}) (\unlift{u})
	\stackrel{(c)}{=} [x](\unlift{s} \unlift{u})
	= [x]\unlift{t} \end{split}\]
and
\[\begin{split} [x]'t' \sap \sivar x = \spure \mathbf{C} \sap [x]'s \sap u \sap \sivar x
	\simeq_{C(A')} [x]'s \sap \sivar x \sap u \\
	\stackrel{\text{(IH)}}{\simeq}_{C(A')} s \sap u = t. \end{split}\]
\end{proof}

Now we can state the key obversation:
The successful abstraction of all variables in an idiomatic term leaves a
single pure term, per the homomorphism law.
Moreover, that term is equivalent to the result of applying the same abstraction
algorithm to the ``unlifted term''.
In principle, this works with arbitrary rules, as long as the statements of
Lemma~\ref{thm:bracket-lifting} hold true.

\begin{theorem}\label{thm:unlifting}
In the following, bracket abstraction uses algorithms $A$ and $A'$ with
a subset of the rules $(i)$--$(t)$ and $(i')$--$(t')$, respectively.
Let $t' \in \mathcal{I}'$ be a generic idiomatic term, and $x_1,\dots,x_n$
a permutation of the variables $\vars(t')$, or a superset thereof.
If $f = [x_1]\cdots[x_n]\unlift{t'}$ is defined for $A$, then
\begin{enumerate}
\item $[x_1]'\cdots[x_n]'t'$ consists only of applications of pure terms, and
\item the unique canonical form of $[x_1]'\cdots[x_n]'t'$ is $\spure f$;
\item $\spure f \sap \sivar x_1 \sap \cdots \sap \sivar x_n$ is a canonical
	form of $t'$;
\item replacing all combinators from $C(A)$ in $f$ with their definitions
	yields $f' =_{\beta\eta} \abs{x_1\cdots x_n}{\unlift{t'}}$.
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
\item is due to $\unlift{([x_1]'\cdots[x_n]'t')} = f$ (induction and
	Lemma~\ref{thm:bracket-lifting}).
\item It is not difficult to see that a pure-only term $p$ has a unique canonical
	form, which is equal to $\spure \unlift{p}$.
\item We have $\spure f \sap \sivar x_1 \sap \cdots \sap \sivar x_n \simeq_{C(A)} t'$
	by induction, making repeated use of Lemma~\ref{thm:bracket-lifting}.
\item We show $[x_1]\cdots[x_n]\unlift{t'} =_{\delta\beta} \abs{x_1\cdots x_n}{\unlift{t'}}$.
	Note that $([x_i]p)x_i =_{\delta\beta} p$ for all terms $p$, and hence
	$[x_i]p =_\eta \abs{x_i}{([x_i]p)x_i} =_{\delta\beta} \abs{x_i}{p}$.
\end{enumerate}
\end{proof}

\begin{table}\centering
\begin{tabular}{lll} Base & Abstraction & Example idioms \\
\hline
$\mathbf{BI}$ & $(ibt)$ & state, list \\
$\mathbf{BIC}$ & $(ibtc)$ & set \\
$\mathbf{BIK}$ & $(kibt)$ & \\
$\mathbf{BIW}$ & $(ibtw)$ & either \\
$\mathbf{BCK}$ & $(kibtc)$ & \\
$\mathbf{BKW}$ & $(kibtw)$ & \\
$\mathbf{BICW}$ & $(ibtcs)$ & maybe \\
$\mathbf{BCKW}$ & $(kibtcs)$ & stream, $\alpha \to$ \\
\end{tabular}
\caption{Substructures of BCKW.}
\label{tab:combinator-bases}
\end{table}

Remember that we are interested in equations, which obviously consist of two
idiomatic terms.
We get to the base equation only if the same variable sequence is used for both
terms, and the assumptions of Theorem~\ref{thm:unlifting} are satisfied.
To complete the \emph{generic lifting} approach, we need a procedure for
determining the abstraction order.
Since this procedure has to depend on the abstraction algorithm, we fix the
combinator bases first.
Hinze focuses on $\mathbf{SK = BCKW}$ and $\mathbf{BICS = BICW}$, noting
that $\mathbf{BIC}$ is also relevant.
The set $\{\mathbf{B,I,C,K,W}\}$ and its subsets seem to be a good starting
point to cover relevant cases.
The additional combinators play an intuitive role:
$\mathbf{C}$ reorders variables, $\mathbf{W}$ duplicates them, and $\mathbf{K}$
permits abstraction over additional variables.
Table~\ref{tab:combinator-bases} lists all distinct subsets containing
$\mathbf{B}$ and $\mathbf{I}$, together with the abstraction algorithms we
propose.
We routinely ignore $\mathbf{T}$ when listing the combinators.
This highlights the connection with combinatorial logic, where some of those
bases have been studied.

The algorithms have been chosen in order to simplify the implementation, by
using the rules conditionally depending on the available combinators:
If $\mathbf{K}$ exists, start with $k$.
Then, for all bases, perform $ibt$.
If $\mathbf{C}$ (or $\mathbf{W}$) exists, add $c$ (or $w$), respectively.
However, if both do, use rule $s$ instead.
Below follows a detailed description of how the variable sequence can be found
in each base, and we justify the abstraction algorithms, meaning that the
preconditions of Theorem~\ref{thm:unlifting} are satisfied.
The function $\abseq_C(s,t)$ will denote the chosen sequence for terms $s$, $t$
in the context of base $C$.
% TODO talk about the examples in the table

\subsubsection*{$\mathbf{BI}$}\label{subsec:base-bi}

This is the minimal base which is available for all idioms.
We already know from Section~\ref{subsec:idiomatic-calculus} that there is only
one canonical form with respect to $\simeq$.
Therefore, there is exactly one permissible sequence:
\[ \abseq_\mathbf{BI}(s,t) = \varseq(s) = \varseq(t). \]
Equations where the two sequences differ are rejected.
If any other sequence could be used, we would get a different canonical form
per Theorem~\ref{thm:unlifting}, thus contradicting the previous result on
normal forms.
Focusing on a single abstraction step $[x_i]t_i$, $x_i$ must occur once in
$t_i$, and it is the right-most variable.
If $t_i$ is an application, there are two cases:
$x_i$ occurs in the right subterm, and rule $(b)$ is used successfully.
Otherwise, there can be no variable in the right subterm, so $(t)$ applies.
This confirms that algorithm $(ibt)$ is indeed acceptable for this base.

\subsubsection*{$\mathbf{BIC}$}\label{subsec:base-bic}

Bunder shows~\cite{bunder96} that $\mathbf{BIC}$-definable lambda terms are
those where each bound variable occurs excatly once, irrespective of their order.
His definition of a $C$-definable term $t$ (with combinator base $C$)
implies that there exists an abstraction algorithm such that $[x]s$ is defined
if $t = \abs{x}{s}$.
In particular, $(i\eta bc)$ is a valid algorithm.
From this it follows that we can choose the order in which we abstract, as long
as the corresponding variable occurs exactly one.
Note that our special $\mathbf{T}$ combinator is not considered there.
But as it can be simulated by the more powerful $\mathbf{CI}$, adding it to the
combinator base does not change anything.

In this base, we can work with all equations where the variable sequence of
one term can be reordered to the sequence of the second.
The order used for the abstraction is irrelevant, but it will be reflected
in the quantifier order of the base equation.
A simple choice is $\abseq_\mathbf{BIC}(s,t) = \varseq(s)$.
However note that we do not use the $(\eta)$ rule, but add the $(t)$ rule.
$(\eta)$ could be a considered as an optimization, since
$\abs{x}{yx} =_\beta \mathbf{B}y\mathbf{I}$, so $(b)$ and $(i)$ suffice.
Conversely, $(t)$ is a special case of $(c)$ with $(i)$, and adding it to the
algorithm next to $(c)$ just results in a slightly different combinator
representation.
We use this particular algorithm in order to simplify the implementation,
such that as much code as possible can be shared between the bases. 

\subsubsection*{$\mathbf{BCK}$ and $\mathbf{BICW}$}\label{subsec:base-bck-bicw}

These cases were also analyzed by Bunder.
For $\mathbf{BCK}$, the definable terms are those where each bound variable occurs
at most once, again ignoring the order.
In terms of bracket abstraction, $[x]y$ is then also defined if $x$ is not free
in $y$.
This allows us to extend the sequence with other variables.
We make use of this to deal with variables which only occur on one side of
an equation---it does not hurt to abstract over those too.
Hence, $\abseq_\mathbf{BCK}(s,t)$ can be any arrangement of the
set $\vars(s) \cup \vars(t)$.
The implementation uses a total order on the set of variables.
On the contrary, the definable terms of $\mathbf{BICW}$ have at least one
occurence of each bound variable; $[x]y$ can therefore be used if $x$ occurs
multiple times in $y$, and $x$ will not be free in $[x]y$.
Comparing this with $\mathbf{BIC}$, we loosen the restriction that variables
must not be repeated.
$\abseq_\mathbf{BICW}(s,t)$ must be an arrangement of $\vars(s) = \vars(t)$.
We can compute this by sorting the sequences again, and then trimming duplicates
which now are next to each other.

Bunder proposes the $(i\eta kbc)$ algorithm for $\mathbf{BCK}$, and
$(i\eta bcs)$ for $\mathbf{BICW}$; our rules are $(kibtc)$ and $(ibtcs)$,
respectively.
The comments on $(\eta)$ and $(t)$ from above apply here as well.
The only other difference is the position of $(k)$ in the list.
But $(k)$ on one side and $(i)$ and $(\eta)$ on the sider work with disjoint
sets of terms, so this is not an issue.

\subsubsection*{$\mathbf{BCKW}$}\label{subsec:base-bckw}

This base, which is logically equivalent to $\mathbf{SK}$, has some useful
properties:
$[x]y$ can always be defined, making it possible to use any abstraction
sequence, and in turn handle all equations.
In conjunction with Theorem~\ref{thm:unlifting}, we want to abstract all
free variables, though.
Therefore we determine the sequence as with $\mathbf{BCK}$, but the
term restriction is rescinded.
As for the abstraction algorithm, we use a variation of $(ik\eta bcs)$
from~\cite{curry68}.

\subsubsection*{$\mathbf{BIK}$, $\mathbf{BIW}$ and $\mathbf{BKW}$}\label{subsec:base-odd}

These bases do not appear to be significantly covered in the literature.
Since there is at least one useful example of an idiom with $\mathbf{BIW}$
combinators, we still shall implement and discuss them.
By adding the $\mathbf{K}$ combinator to $\mathbf{BI}$, additional variables
may be abstracted, but the other restrictions (order, single occurrence) remain.
Accordingly, we demand that $\abseq_\mathbf{BIK}(s,t)$ has $\varseq(s)$ and
$\varseq(t)$ as subsequences.
% TODO define "subsequence" somewhere?
For definiteness, we can limit it to the shortest sequence where variables
only in $s$ appear before those only in $t$, whenever there is an ambiguity:
$\abseq_\mathbf{BIK}(s,t) = \abseq'_\mathbf{BIK}(\varseq(s), \varseq(t))$,
\[ \abseq'_\mathbf{BIK}(a,b) = \begin{cases}
		x \cdot \abseq'_\mathbf{BIK}(a',b') &\text{if } a = x \cdot a', \; b = x \cdot b'; \\
		x \cdot \abseq'_\mathbf{BIK}(a',b) &\text{if } a = x \cdot a', \\
		x \cdot \abseq'_\mathbf{BIK}(a,b') &\text{if } b = x \cdot b', \\
		\langle \rangle &\text{if } a = b = \langle \rangle. \end{cases} \]
Trying rule $(k)$ in the beginning of the abstraction algorithm obviously takes
care of unused variables.

For the other bases, we simply take the abstraction algorithms as granted.
Based on that we analyze the set of defined bracket abstractions.
Since $\mathbf{W}$ duplicates variables, one might conjecture that a single
variable may now occur repeatedly.
No other variable may be interspersed in the lexical order, though, because
none of the available combinators reorder their arguments.
The following lemma proves that this intuition is correct.
In order to express it on the level of lambda terms, we overload $\varseq(x)$
in the obvious way.
We use the notation $x^n$ with sequence $x$ to stand for $n$ concatenated
copies of $x$.

\begin{lemma}
Let $x \in \mathcal{V}$ and $t \in \mathcal{T}$.
With algorithm $(ibtw)$, $[x]t$ is defined iff there is a natural number
$n \geq 1$ and a variable sequence $v$ such that
$\varseq(t) = v \mathbin{@} \langle x \rangle^n$, where $x$ does not appear in $v$.
The same statement holds true for algorithm $(kibtw)$, except that $n$ may be
zero as well.
\end{lemma}
\begin{proof}
The first part of the proof is concerned with the direction where $[x]t$ is
assumed to be defined.
We perform induction over the steps of the abstraction algorithm.
In the evaluation of $[x]s$ for a subterm $s$ of $t$, there are the following
cases:
If $(k)$ is used, then $x$ must not be free in $s$, thus $n = 0$ and $v = \varseq(s)$.
For $(i)$, we have $n = 1$ and $v = \langle\rangle$.
For $(b)$, there must be terms $u$ and $w$ with $s = uw$, and $x$ is not free
in $u$.
From the induction hypothesis we get adequate $n'$ and $v'$ for $[x]w$.
Then $n'$ and $\varseq(u) \mathbin{@} v'$ satisfy the conditions for $[x]s$.
Rules $(t)$ and $(w)$ are analogous:
For $(t)$, the side condition implies that $\varseq(w) = \langle\rangle$;
for $(w)$ we have $\varseq(w) = \langle x \rangle^k$ for some $k \geq 1$,
thus $\varseq(u) \mathbin{@} \varseq(w) = v' \mathbin{@} \langle x \rangle^{n'+k}$,
where $n'$ and $v'$ are from $[x]u$.

Now we show the other direction.
Assume that suitable $n$ and $v$ exist.
If $n = 0$, rule $(k)$ is used, because $x$ cannot be free in $t$.
Otherwise we assume $n \geq 1$ during an induction on the structure of $t$.
If $t$ is a variable, it must be $x$, so rule $(i)$ applies.
If it is an application $s = uw$ instead, there are three cases:
$x \not\in \vars(u)$, hence $v = \varseq(u) \mathbin{@} v'$ for some $v'$,
and $[x]w$ is defined by the induction hypothesis so we can use rule $(b)$.
If $\vars(w) = \emptyset$, $[x]u$ is defined and rule $(t)$ applies.
Otherwise, $\varseq(w) = \langle x \rangle^k$ with $k \leq n$ must hold,
so $(w)$ can be used.
\end{proof}

It follows that the abstraction sequences for $\mathbf{BIW}$ and $\mathbf{BKW}$
should be chosen like those of $\mathbf{BI}$ and $\mathbf{BIK}$, respectively;
but for each variable, a single span of repetitions in $\varseq(s)$ and
$\varseq(t)$ is allowed and treated like a single instance.

\subsection{Implementation}\label{subsec:combinator-implementation}

\todo
